{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniforge-pypy3/envs/envv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import DonutProcessor\n",
    "from datasets import load_dataset, Dataset\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"naver-clova-ix/cord-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "tokenizer = processor.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    added_tokens = set()\n",
    "    def json2token(obj, update_special_tokens_for_json_key, sort_json_key: bool = True):\n",
    "        \"\"\"\n",
    "        Convert an ordered JSON object into a token sequence\n",
    "        \"\"\"\n",
    "        if type(obj) == dict:\n",
    "            if len(obj) == 1 and \"text_sequence\" in obj:\n",
    "                return obj[\"text_sequence\"]\n",
    "            else:\n",
    "                output = \"\"\n",
    "                if sort_json_key:\n",
    "                    keys = sorted(obj.keys(), reverse=True)\n",
    "                else:\n",
    "                    keys = obj.keys()\n",
    "                for k in keys:\n",
    "                    if update_special_tokens_for_json_key:\n",
    "                        tokenizer.add_tokens([fr\"<s_{k}>\", fr\"</s_{k}>\"])\n",
    "                        added_tokens.add(fr\"<s_{k}>\")\n",
    "                        added_tokens.add(fr\"</s_{k}>\")\n",
    "                    output += (\n",
    "                        fr\"<s_{k}>\"\n",
    "                        + json2token(obj[k], update_special_tokens_for_json_key, sort_json_key)\n",
    "                        + fr\"</s_{k}>\"\n",
    "                    )\n",
    "                return output\n",
    "        elif type(obj) == list:\n",
    "            return r\"<sep/>\".join(\n",
    "                [json2token(item, update_special_tokens_for_json_key, sort_json_key) for item in obj]\n",
    "            )\n",
    "        else:\n",
    "            obj = str(obj)\n",
    "            if obj in added_tokens:\n",
    "                obj = f\"<{obj}/>\"  # for categorical special tokens\n",
    "            return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:06<00:00, 14.78it/s]\n",
      "100%|██████████| 800/800 [00:58<00:00, 13.60it/s]\n",
      "100%|██████████| 100/100 [00:07<00:00, 13.92it/s]\n"
     ]
    }
   ],
   "source": [
    "image_id = 0\n",
    "for split in [\"test\", \"train\", \"validation\"]:\n",
    "    with open(f\"cord_{split}.jsonl\", 'w') as f:\n",
    "        dataset_split = dataset[split]\n",
    "        ground_truths = dataset_split[\"ground_truth\"] \n",
    "        images = dataset_split[\"image\"]\n",
    "        sz = len(ground_truths)\n",
    "        for i in tqdm(range(sz)):\n",
    "            line = {}\n",
    "            line[\"task\"] = \"cord\"\n",
    "            images[i].save(f\"cord_images/{image_id}.jpg\")\n",
    "            line[\"image_path\"] = f\"cord_images/{image_id}.jpg\"\n",
    "            image_id += 1\n",
    "            line[\"ground_truth\"] = json.dumps(json.loads(ground_truths[i])[\"gt_parse\"])\n",
    "            gt_tokens = json2token(json.loads(ground_truths[i])[\"gt_parse\"], True)\n",
    "            labels = tokenizer(gt_tokens + \"</s>\", add_special_tokens=False).input_ids\n",
    "            line[\"labels\"] = labels\n",
    "            if split == \"train\":\n",
    "                line[\"input_ids\"] = tokenizer(\"<s>\" + gt_tokens, add_special_tokens=False).input_ids\n",
    "            else:\n",
    "                line[\"input_ids\"] = tokenizer(\"<s>\", add_special_tokens=False).input_ids\n",
    "            f.write(json.dumps(line) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cord_tokenizer/tokenizer_config.json',\n",
       " 'cord_tokenizer/special_tokens_map.json',\n",
       " 'cord_tokenizer/sentencepiece.bpe.model',\n",
       " 'cord_tokenizer/added_tokens.json',\n",
       " 'cord_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"cord_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 3/3 [00:00<00:00, 6105.25it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 1361.49it/s]\n",
      "Generating train split: 800 examples [00:00, 40829.42 examples/s]\n",
      "Generating test split: 100 examples [00:00, 23248.73 examples/s]\n",
      "Generating validation split: 100 examples [00:00, 19240.81 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"json\", data_files={\"train\": \"cord_train.jsonl\", \"test\": \"cord_test.jsonl\", \"validation\": \"cord_validation.jsonl\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 122.11ba/s]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 563.83ba/s]\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:09<00:00,  9.13s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 509.08ba/s]\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='hf-hub-lfs-us-east-1.s3.us-east-1.amazonaws.com', port=443): Max retries exceeded with url: /repos/da/2e/da2e3325cca3dc46dd681a6573ef98c743e93c1467ba69d8ec0b5520ebdefdd6/8d611b70910dd56ea3a3a8c433add419a58dd10b40fbbe9c19d3cb5a06e8cb91?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQFN2FTF47%2F20231107%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231107T044451Z&X-Amz-Expires=900&X-Amz-Signature=0d6eda50e496f47cdb9e7179dc3d7951ec577a024eaaba881e299769e44d9d5b&X-Amz-SignedHeaders=host&x-amz-storage-class=INTELLIGENT_TIERING&x-id=PutObject (Caused by ProxyError('Unable to connect to proxy', RemoteDisconnected('Remote end closed connection without response')))\"), '(Request ID: 29bad9da-24d9-4da7-a05e-fe81e68cb1eb)')' thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3.us-east-1.amazonaws.com/repos/da/2e/da2e3325cca3dc46dd681a6573ef98c743e93c1467ba69d8ec0b5520ebdefdd6/8d611b70910dd56ea3a3a8c433add419a58dd10b40fbbe9c19d3cb5a06e8cb91?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQFN2FTF47%2F20231107%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231107T044451Z&X-Amz-Expires=900&X-Amz-Signature=0d6eda50e496f47cdb9e7179dc3d7951ec577a024eaaba881e299769e44d9d5b&X-Amz-SignedHeaders=host&x-amz-storage-class=INTELLIGENT_TIERING&x-id=PutObject\n",
      "Retrying in 1s [Retry 1/5].\n",
      "Pushing dataset shards to the dataset hub: 100%|██████████| 1/1 [00:04<00:00,  4.98s/it]\n",
      "Downloading metadata: 100%|██████████| 811/811 [00:00<00:00, 3.79MB/s]\n"
     ]
    }
   ],
   "source": [
    "ds.push_to_hub(\"cord_donut_multitask\", token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envv",
   "language": "python",
   "name": "envv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
